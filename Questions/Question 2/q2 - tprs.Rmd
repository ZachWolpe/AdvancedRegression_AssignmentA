---
title: 'Question 2: Thin Plate Splines'
output:
  html_document:
    df_print: paged
---



# Advanced Regression Assignment A

##### Zach Wolpe                                 
##### WLPZAC001
_01 June 2020_

------------------


```{r, warning=F, error=F, echo=F}
#setwd("~/Desktop/MSc Advanced Analytics/Advanced Regression/Assignments/AR Assignment 1")
#setwd("~/Desktop/MSc Advanced Analytics/Advanced Regression/Assignments/AR Assignment 1")
library(ggplot2)
library(splines)
library(scatterplot3d)
# library(rgl)
library(GA)
library(mgcv)
library(plotly)
```

## Generate the data

The data is generated on a unit square, where $x$ & $z$ $100$ equally spaced points between $0-1$ & $y=f(x,z)$ is generate as a function of $x$ & $z$:

![](/Users/zachwolpe/Desktop/MSc Advanced Analytics/Advanced Regression/Assignments/AR Assignment 1/final implementation/Images/q2 equation.png)

Noise is then added to the data:

$$e \sim N(0,0.1^2)$$

Here we visualize both the true & the noisy function.

```{r, echo=F, warning=F, error=F}
x <- seq(0,1,length.out = 100)
z <- seq(0,1,length.out = 100)
sig_x <- 0.3; sig_z <- 0.4

fx <- function(x, z, sig_x=sig_x, sig_z=sig_z) {
  0.75/(pi*sig_x*sig_z) * exp( -((x-0.2)^2)/(sig_x^2) - ((z-0.3)^2)/(sig_z^2)) +
  0.45/(pi*sig_x*sig_z) * exp( -((x-0.7)^2)/(sig_x^2) - ((z-0.8)^2)/(sig_z^2)) 
}


# ---- Question 1 ---- x
signal <- fx(x,z, sig_x, sig_z)
sig <- signal + rnorm(n=100, mean=0, sd=0.1)
y <- sig



pp <- expand.grid(x=x, z=z)
y_expand <- fx(pp$x,pp$z, sig_x, sig_z)
y_expand <- matrix(y_expand, nrow = 100)


image(x, z, y_expand, col = heat.colors(20), main='True Function')     
contour(x, z, y_expand, add = F, nlevels = 20, main='True Function')
persp(x, z, y_expand, theta = 10, phi = 25, main='True Function')
persp3D(x,z,y_expand, theta=10, phi=25, main='True Function')
plot_ly(x = x, y = z, z = y_expand, title='True Function') %>% add_surface()




pp <- expand.grid(x=x, z=z)
y_expand <- fx(pp$x,pp$z, sig_x, sig_z) +  rnorm(n=10000, mean=0, sd=0.1)
y_expand <- matrix(y_expand, nrow = 100)


image(x, z, y_expand, col = heat.colors(20), main='True Function with Noice')     
contour(x, z, y_expand, add = F, nlevels = 20, main='True Function with Noice')
persp(x, z, y_expand, theta = 10, phi = 25, main='True Function with Noice')
persp3D(x,z,y_expand, theta=10, phi=25, main='True Function with Noice')
plot_ly(x = x, y = z, z = y_expand, title='True Function') %>% add_surface()


```

------------------

## Comparison

We want to compare two modeling techniques, by their ability to approximate the true functional form:
  - $16$ evenly spaced knots to construct on thin spline basis
  - Construct a full rank thin spline basis & use eigenvalue deconstruction to produce a $16$ rank basis 


### Random Sample

$10000$ points are generated to fit the true functional form, however the models are trained on $100$ random samples from the the $10000$ available points.


### Evenly spaced Knots & Radial Basis Approach

Choose 16 evenly spaced knots, construct a thin-plate spline basis, and fit this to the data (unpenalized).

Radial Basis Fuctions:

$$h_j(x) = ||x-x_j||^2 log||x-x_j||$$

if $u$ is the euclidean distance between $2$ points, this reduces to:

$$r(u) = u^2 log(u)$$

Knot location are generated by: 
  - placing $4$ evenly spaced knots on each axes
  - using each combination as a point on the $2$ dim plane
  - resulting in $16$ evenly spaced locations on the $2$-d plane 
  


```{r, echo=F, warning=F, error=F}

# ---- Question 2 ----x

# ---- generate knots ----x
knots_x <- seq(0,1, length.out = 4)
knots_z <- seq(0,1, length.out = 4)


# ---- sample ----x
set.seed(1)
index <- sample(1:10000, size=100)
xz <- pp[index,]
y <- fx(xz[,1],xz[,2], sig_x, sig_z) + rnorm(n=100, mean=0, sd=0.1)




# radial basis functions 
compute_distance <- function(xz, knots_x, knots_z, include_int=F) {
  basis <- c()
  x <- xz[,1]; z <- xz[,2]
  if (include_int) basis <- rep(1,length(x))
  for (i in 1:length(knots_x)) {
    for (j in 1:length(knots_z)) {
      u <- sqrt((x-knots_x[i])^2 + (z-knots_z[j])^2)
      if (u==0) basis <- cbind(basis, rep(0, length(x)))
      else basis <- cbind(basis, u^2 * log(u))
    }
  } 
  basis
}




# ---- fit model ----x 
basis <- compute_distance(xz,knots_x,knots_z)
model <- lm(y ~ basis)
summary(model)




# ---- visualize ----x 

basis_2 <- compute_distance(pp, knots_x, knots_z, T)

beta <- coef(model)
beta[is.na(beta)] <- 0
yhat_plot <- basis_2 %*% beta
yhat_plot <- matrix(yhat_plot, nrow = 100)




image(x, z, yhat_plot, col = heat.colors(20), main='16 Knot Radial Basis')     
contour(x, z, yhat_plot, add = F, nlevels = 20, main='16 Knot Radial Basis')
persp(x=x, y = z, z = yhat_plot, theta = 10, phi = 25, main='16 Knot Radial Basis')
persp3D(x, z, yhat_plot, theta=10, phi=25, main='16 Knot Radial Basis')
plot_ly(x=x, y=z, z = yhat_plot, title='16 Knot Radial Basis') %>% add_surface()


```


The results are of the evenly spaced knots radial basis spline do capture the general trend/curve in the data. The double-bump structure is captured to some degree.


$16$ evenly spaced Radial basis fit captures the true functional form in a meaningful, accurate, way. The general trend is acheived & two bumps do emerge (as in the true function). Reduced Rank thin plate splines may offer an alternative to intelligently select knot bases locations (by variance explained) to better capture the data.  


-------------

## Reduced Rank Thin Plate Spline Approach

As an alternative to the aforementioned tecnique, a full rank thin plate spline basis is computed & thereafter the rank is reduced by selecting only the desired $k=16$ basis functions that explain the maximum variance - computed via eigenvalue decompositition.

The implementation, adapted from [1], is given as follows:


![](/Users/zachwolpe/Desktop/MSc Advanced Analytics/Advanced Regression/Assignments/AR Assignment 1/final implementation/Images/tprs low rank algorithm.png)


Once matrices $E$ & $T$ are computed, the algorithm is straight forward. 

$T$ contains basis functions for constant and linear terms: $M= {m+d-1 \choose m}$ functions are linearly independent polynomials spanning the space of polynomials with degree less than $m$ (pg. 97).

$E$ contains radial basis functions associated with knots, initiated as a full rank matrix $(n \times n)$ thereafter the eigenvalue decomposition is used to select the $K$ basis vectors that account for the most variation.

Once the bases & penalty matrix are computed, the model is fit using a standard penalized least squares apprach, resulting in a parameter estimate:

$$ \hat{\beta} = (X'X+\lambda P)^{-1}X'y $$

if the model is penalized (by $S$) or:

$$ \hat{\beta} = (X'X)^{-1}X'y $$

If the penalty is ignored.

#### Hyperparameters

This algorithm requires a few hyperparameters, the initial chosen values are:
    $d=2$: dimensions of the surface ($d$ covariates)
    $m=2$: order of differentiation in wiggliness penalty
    $k=16$: rank of basis matrix
    $M=3$


-------------

## Reduced Rank Thin Plate Splines Implementation 

It's not clear what specific parameter configuration will fit the best Reduced Rank Think Plate Spline (RR-TPS), so I wrote a function that fits the above algorithm but allows one to tweak a few parameters:
  - Penalized fit or Unpenalized fit 
  - $\lambda$ to be used if the $S$ penality is implemented
  - Eigen value decomposition or singular value decomposition to compute the basis matrices
  
  
The _EVD or SVD_ parameter option was added, although _EVD_ was implemented here.


### Fit 1

Compute a RR-TPS using EVD, where T constitutes polynomials up until the $1^st$ degree. The model is fit using unpenalized least squares.

```{r, echo=F, warning=F, error=F}

library(plotly)
library(MASS)
library(future.apply)
library(stats)
library(RSpectra)
library(ggplot2)
library(splines)
library(scatterplot3d)
# library(rgl)
library(GA)
library(mgcv)

library(corpcor)

### Z dim
### T dim

# ---- sample ----x
set.seed(12234)
index <- sample(1:10000, size=100)
xz <- pp[index,]
y <- fx(xz[,1],xz[,2], sig_x, sig_z) + rnorm(n=100, mean=0, sd=0.1)




# ===============
fit_thin_plate_pline <- function(t_fuction, size, lambda=1, penalize=F, eigen_or_svd=T) {
  "
  Return:     Parameter coefficients for Thin Plate Spline
              Visualization of Thin Plate Spline
              
  Arguments:  
      t_fuction:          Function that produces the T matrix
      size:               Size^2 = number of points for visualization (computation time)
      lambda:             Penalization coefficient
      penalize:           Logical: fit penalized or non-penalized model
      eigen_or_svd:       use Eigenvalue decomposition or Singular value decomposition
                          T: eigenvalue decomp
                          F: singular value decomp
  "
  
  construct_basis_reduced_rank <- function(xz,t) {
    
    x <- xz[,1]; z <- xz[,2]
    
    knots_x <- seq(0,1, length.out = sqrt(length(x)))
    knots_z <- seq(0,1, length.out = sqrt(length(z)))
    
    
    knot_grid <- expand.grid(knots_x=knots_x, knots_z=knots_z)
  
    # ---- compute E ---- x
    
    # ---- Compute Vector: 1 Column in Basis Matrix ----x
    
    compute_distance_vec <- function(x, z, knot_index) {
      u <- sqrt((x-knot_grid[knot_index,1])^2 + (z-knot_grid[knot_index,2])^2)
      u^2 * log(u)
    }
    
    # compute_distance_vec <- function(xz, knot_index) {
    #   x <- xz[,1]; z <- xz[,2]
    #   u <- sqrt((x-knot_grid[knot_index,1])^2 + (z-knot_grid[knot_index,2])^2)
    #   u^2 * log(u)
    # }
    
    
    basis <- future_lapply(FUN = compute_distance_vec, x=xz[,1], z=xz[,2], X = 1:nrow(knot_grid))
    basis <- matrix(unlist(basis), nrow=length(x))
    basis[is.na(basis)] <- 0
    E <- basis 


    
    
    # ---- Eigen or SVD ----x
    evd_or_svd <- function(eigen_or_svd) {
      if (eigen_or_svd) {
        # ---- eigen value decomp ----x
        evd <- eigs(A=E, k=16)
        U <- evd$vectors
        D <- diag(evd$values)
        
        U16 <<- U[,1:16]
        D16 <<- diag(evd$values[1:16])
      }
      else {
        # ---- SVD decomp ----x
        # x.svd <- svd(E)
        x.svd <- fast.svd(E)
        D <- x.svd$d
        U <- x.svd$u
        V <- x.svd$v
        
        U16 <<- U[,1:16]
        D16 <<- diag(D[1:16])

      }
    }
    
    evd_or_svd(eigen_or_svd)
    
    
    
    
    # ---- keep k=16 highest variance eigenvectors ----x
    QR <- qr(t(U16) %*% t)
    Q <- qr.Q(QR, complete = t)
    R <- qr.R(QR)
    Z <- Q[,size_z:16]
  
    # ---- Design Matrix ----x
    Xnew <- cbind(U16 %*% D16 %*% Z, t)

    list(Xnew=Xnew, Z=Z, D16=D16)
  }
  
  
  t <- t_fuction(x, z)
  size_z <- ncol(t)
  res <- construct_basis_reduced_rank(xz,t)
  Xnew <- res$Xnew
  Z <- res$Z
  D16 <- res$D16
  
  
  
  # ---- Add Penalty Matrix ----x
  s <- dim(Xnew)[2]
  S <- matrix(0, nrow=s, ncol=s)
  if (penalize) {
    # target size --> s
    g <- dim(t(Z) %*% D16 %*% Z)[1] 
    S <- rbind(cbind(t(Z) %*% D16 %*% Z, matrix(rep(0, (s-g)*g), ncol=(s-g))), 
             matrix(rep(0, (s-g)*s), nrow=(s-g)))
  }
  
  
  # ---- Fit Model ---- x
  beta_pls <- ginv(t(Xnew) %*% Xnew + lambda*S) %*% t(Xnew) %*% y
  beta_pls[is.na(beta_pls)] <- 0
  yhat <- Xnew %*% beta_pls
  
  
  results <- list(design_matrix=Xnew, S=S, Z=Z, D16=D16, yhat=yhat, beta=beta_pls)


  
  # ---- Fit Visualization Data ---- x
  xx <- seq(0,1,length.out = size)
  zz <- seq(0,1,length.out = size)
  grid <- expand.grid(x=xx,z=zz)
  t <- t_fuction(grid$x, grid$z)
  # res <- construct_basis_reduced_rank(grid$x,grid$z,t)
  res <- construct_basis_reduced_rank(grid,t)
  yhat <- res$Xnew %*% beta_pls
  yhat <- matrix(as.numeric(yhat), nrow=size)
  
  
  # ---- Visualize ---- x
  xx <- seq(0,1,length.out = (size))
  zz <- seq(0,1,length.out = (size)) 

  image(x=xx, y=zz, z=yhat, col = heat.colors(20), main='Thin Plate Spline')     
  contour(xx, zz, z = yhat, add = F, nlevels = 20, main='Thin Plate Spline')
  persp(x=xx, y = zz, z = yhat, theta = 10, phi = 25, col='steelblue', main='Thin Plate Spline')
  persp3D(xx, zz, yhat, theta=10, phi=25, main='Thin Plate Spline')
  fig <- plot_ly(x = xx, y = zz, z = yhat) %>% add_surface()
  fig
  
  results <- append(results, list(fig=fig))
  results
}




gen_t <- function(x,z) {
  matrix(cbind(rep(1, length(x)), x,z), ncol=3)
}

x <- xz[,1]; z <- xz[,2]
res <- fit_thin_plate_pline(t_fuction = gen_t, size=100, lambda = 1, penalize = F, eigen_or_svd =T)
res$fig


```


The model fits the data in a very simplistic mannar - capturing overall structure but no detail (producing 1 large bump as apposed to the true known functional form). The data appears to flair up one one side - perhaps a consequence of the particular random sample. The fit is undoubtedly worse than the standard 16-evenly spaced knots.

### Fit 2

The fit may be improved by tweeking the hyper-parameters. Now, instead of fitting un unpenalized model, we fit a penalized least squares to the data. $\lambda$ is arbitrarily chosen to be $1$.


```{r, echo=F, warning=F, error=F}

res <- fit_thin_plate_pline(t_fuction = gen_t, size=100, lambda = 1, penalize = T, eigen_or_svd =T)
res$fig

```


It's evident that the penalization reduces some of the violent movements in the fitted curve - & visually appears to give a more generalizable fit.

### Fit 3

The model maybe improved further by allowing for more degrees of freedom. The $T$ matrix was previously specified to be a consisting of polynomias up until the $1^st$ degree, now lets allow polynomials up until the $2^{st}$ degree - first fitting a non-penalized least squares.

```{r, echo=F, warning=F, error=F}

gen_t <- function(x,z) {
  matrix(cbind(rep(1, length(x)), x,z,x^2,z^2), ncol=5)
}

res <- fit_thin_plate_pline(t_fuction = gen_t, size=100, lambda = 1, penalize = F, eigen_or_svd =T)
res$fig

```



### Fit 4

The additional degrees of freedom appear to allow for a more natural fit, closer to the true functional form. Now we'll fit the same model as above but under _pls_ - again $\lambda$ is arbitrarily chosen to be $1$.



```{r, echo=F, warning=F, error=F}
res <- fit_thin_plate_pline(t_fuction = gen_t, size=100, lambda = 1, penalize = T, eigen_or_svd =T)
res$fig

```


This is the best fit so far, these hyperparameters appear to allow the function to approximate the actual true function to a reasonable degree.

### Fit 5

For good measure, a final fit is performed where the T matrix consists of polynomials up until the $3^{rd}$ degreed, fit by pls.

```{r, echo=F, warning=F, error=F}

gen_t <- function(x,z) {
  matrix(cbind(rep(1, length(x)), x,z,x^2,z^2,z^3,x^3), ncol=7)
}

res <- fit_thin_plate_pline(t_fuction = gen_t, size=100, lambda = 1, penalize = T, eigen_or_svd =T)
res$fig


```

The excessive degrees of freedom seem to allow the model to overfit on the particular sample of training data available. This probably approximates the sample very closely & would not generalize well.

------------------

## Conclusions

The intelligently spaced Reduced Rank Thin Plate Spline - given the correct hyperparameters - appears to have the flexibility to approximate a function well, however is the data (in our case sample of only $100$ points from $10000$) is too small it may overfit. The simplictic 16 evenly spaced knots seems to work well & it is not clear from my experiment if the additional complexity is worth it.











#### References


1. Simon N. Wood, 2003.
"Thin plate regression splines," Journal of the Royal Statistical Society Series B, Royal Statistical Society, vol. 65(1), pages 95-114, February.




